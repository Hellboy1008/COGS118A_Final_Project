{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COGS 118A Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjDmWCbhYEft"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yO-JtzrktOs"
      },
      "source": [
        "# imports\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, jaccard_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsovGd2-mZ6a"
      },
      "source": [
        "## 1. Adult Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_bcB8G1YF2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "outputId": "a84d4876-a7b9-4310-8406-940abce434e2"
      },
      "source": [
        "# read csv data from Github\n",
        "original_adult_df =  pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/adult.data', na_values='?', sep=',', skipinitialspace=True, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
        "original_adult_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvIu1ImOkqkv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "55219d82-9534-4c64-960d-30cb5d55b79c"
      },
      "source": [
        "# remove education-num column because it is a repetition of education column in numeric form\n",
        "# remove fnlwgt because it is not related to the income\n",
        "adult_df = original_adult_df.drop(columns=['fnlwgt', 'education-num'])\n",
        "adult_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihBEC-PIpfN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06618bc8-8f12-4a62-db5a-9eca26789d99"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = adult_df.shape[0] - adult_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3AfTtmR9DKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "119a9659-4ccb-493d-fd4a-90134c0a8330"
      },
      "source": [
        "# remove rows with missing values as it is only 7% of the dataset and we still have plenty of data to work with\n",
        "adult_df = adult_df.dropna()\n",
        "adult_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "mrt-WgzqNu91",
        "outputId": "428a6f1d-bc2c-46f8-9fd3-dc45b6429843"
      },
      "source": [
        "# function to plot class balance for dataset\n",
        "def plotBalance(df, label):\n",
        "  label_count = df[label].value_counts()\n",
        "  label_percentage = str(round(label_count.values[0] / df.shape[0] * 100,2)) + '% vs ' + str(round(label_count.values[1] / df.shape[0] * 100,2)) + '%'\n",
        "  sns.barplot(x=label_count.index, y=label_count.values, alpha=0.9)\n",
        "  plt.title('Frequency of the classes (' + label_percentage + ')')\n",
        "  plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "  plt.xlabel(label, fontsize=12)\n",
        "  plt.show()\n",
        "# plot income class balance\n",
        "plotBalance(adult_df, 'income')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOd82e0aT5ft"
      },
      "source": [
        "The binary variable that we'll be evaluating is \"income\" which describes whether or not income for an individual exceeds $50,000 per year. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf4ucvxiH4T0",
        "outputId": "0bb4e10e-8cfe-4a79-be43-43715dfdf703"
      },
      "source": [
        "# function to one hot encode categorical variables\n",
        "def OneHotEncode(df, cols):\n",
        "  ohe = OneHotEncoder(sparse=False)\n",
        "  return ohe.fit_transform(df[cols])\n",
        "\n",
        "# function to scale continuous variables\n",
        "def StandardScale(df, cols):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit_transform(df[cols])\n",
        "\n",
        "# function to change label to binary class\n",
        "def changeBinary(col, zero_class):\n",
        "  col = [0 if x == zero_class else 1 for x in col]\n",
        "  return np.asarray(col)[:, np.newaxis]\n",
        "\n",
        "# apply one hot encoding and standard scaling, and represent <=50K as 0 and >50K as 1 for the income label\n",
        "encode_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "scale_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "adult_data = np.concatenate([StandardScale(adult_df, scale_cols), OneHotEncode(adult_df, encode_cols), changeBinary(adult_df['income'], '<=50K')], axis=1)\n",
        "adult_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A3EeVbrSxbE"
      },
      "source": [
        "## 2. Mushroom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "pOsz5Zh9d6dO",
        "outputId": "4fa7f64d-5577-4bf0-b6ed-e05c27673645"
      },
      "source": [
        "# read csv data from Github\n",
        "original_mushroom_df =  pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/mushroom.data', na_values='?', sep=',', skipinitialspace=True, names=['toxicity', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface--ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'])\n",
        "original_mushroom_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oupq6l_TfKRw",
        "outputId": "e60c7911-e2fa-4e77-e935-6d1dc56eb008"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = original_mushroom_df.shape[0] - original_mushroom_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iEzLHDHFfknw",
        "outputId": "2a60a787-5e9e-42c7-82fb-82b22f07437f"
      },
      "source": [
        "# We find that about 30% of the data have a missing value, which happens to be all for the attribute stalk-root. Instead of removing the data,\n",
        "# we will change the N/A value to the character 'm' for missing.\n",
        "mushroom_df = original_mushroom_df.fillna('m')\n",
        "mushroom_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "hWnL_JAahhcZ",
        "outputId": "c5f9b182-0e18-4575-feb2-97214cc41c75"
      },
      "source": [
        "# plot toxicity class balance\n",
        "plotBalance(mushroom_df, 'toxicity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gki9twIcYHK6"
      },
      "source": [
        "The binary variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcSgxxBYh2tj",
        "outputId": "55f3cf98-284f-45cf-9f1d-26a532c31389"
      },
      "source": [
        "# apply one hot encoding and standard scaling, and represent e as 0 and t as 1 for the toxicity label\n",
        "encode_cols = mushroom_df.drop(columns=['toxicity']).columns\n",
        "mushroom_data = np.concatenate([OneHotEncode(mushroom_df, encode_cols), changeBinary(mushroom_df['toxicity'], 'e')], axis=1)\n",
        "mushroom_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU6rAtc9D0zd"
      },
      "source": [
        "## 3. QSAR Oral Toxicity Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q176RQQfEG0P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "bbc8368a-7c28-41d4-ae8a-0c64d38c77f1"
      },
      "source": [
        "# read csv data from Github\n",
        "cols = []\n",
        "for i in range(1024):\n",
        "  cols.append('binary molecule data ' + str(i+1))\n",
        "cols.append('toxicity')\n",
        "original_qsar_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/qsar_oral_toxicity.csv', sep=';', skipinitialspace=True, names=cols)\n",
        "original_qsar_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viyAtJ4PmggM",
        "outputId": "f3712c3d-4b09-444d-a2d9-410707e0effa"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = original_qsar_df.shape[0] - original_qsar_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "KJHbMumLmm3K",
        "outputId": "6d12f180-794c-45d0-a6b2-ed361e4b7786"
      },
      "source": [
        "# plot toxicity class balance\n",
        "plotBalance(original_qsar_df, 'toxicity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wju6ff3Amyrw",
        "outputId": "942df741-443b-4a17-e876-61e877b03a9c"
      },
      "source": [
        "# represent negative as 0 and positive as 1 for the toxicity label\n",
        "qsar_data = np.concatenate([original_qsar_df.drop(columns=['toxicity']).to_numpy(), changeBinary(original_qsar_df['toxicity'], 'negative')], axis=1)\n",
        "qsar_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ais-QMRFEHCC"
      },
      "source": [
        "## 4. EEG Eye State Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfAjYJeyEM2J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "3371c4af-0db3-4b52-9fad-6c1c8ed98bd7"
      },
      "source": [
        "# read csv data from Github\n",
        "original_eeg_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/EEG%20Eye%20State.arff', sep=',', skipinitialspace=True, names=['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'eye detection'])\n",
        "original_eeg_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CM8nzL-LAkm",
        "outputId": "97e16c02-a9d9-45c1-97a0-4fe0c55ba556"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = original_eeg_df.shape[0] - original_eeg_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "VhKwbK41LC_3",
        "outputId": "02cf8a8f-a090-4a12-e87a-73db4a754c9d"
      },
      "source": [
        "# plot toxicity class balance\n",
        "plotBalance(original_eeg_df, 'eye detection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc-fIiR1LNL-",
        "outputId": "37955ed2-7d69-4cc1-82ba-948ac3c26a97"
      },
      "source": [
        "# apply standard scaling\n",
        "scale_cols = original_eeg_df.drop(columns=['eye detection']).columns\n",
        "eeg_data = np.concatenate([StandardScale(original_eeg_df, scale_cols), original_eeg_df['eye detection'].to_numpy()[:, np.newaxis]], axis=1)\n",
        "eeg_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryzuuSucELzI"
      },
      "source": [
        "## 5. In-vehicle coupon recommendation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY0d47LTES-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "2c469cfe-5f98-4015-c933-928a0b0b2b79"
      },
      "source": [
        "# read csv data from Github\n",
        "original_coupon_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/in-vehicle-coupon-recommendation.csv', sep=',', skipinitialspace=True)\n",
        "original_coupon_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgapB6seOyF3",
        "outputId": "aacc5069-e9b2-494a-872a-88f87060eb11"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = original_coupon_df.shape[0] - original_coupon_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UCHQb4wQe_A",
        "outputId": "73941663-e471-4b6c-ecaa-bf60aad8d73a"
      },
      "source": [
        "# check which columns have missing values\n",
        "original_coupon_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "yI0PXogUQzGW",
        "outputId": "6cbfb7aa-c813-4ded-bdf1-722816b41f59"
      },
      "source": [
        "# drop the data from car column as it is missing values for most of the dataset\n",
        "coupon_df = original_coupon_df.drop(columns=['car'])\n",
        "# drop the rest of the rows with missing values as it is only 4% of the dataset\n",
        "coupon_df = coupon_df.dropna()\n",
        "coupon_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "6jj1poB8RcAY",
        "outputId": "a1339862-d0c3-4546-9bbf-cc6c7563f05e"
      },
      "source": [
        "# plot coupon class balance\n",
        "plotBalance(coupon_df, 'Y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVkmhD64RpN0",
        "outputId": "51f882eb-d6c3-484a-b13f-e57c65eb34bf"
      },
      "source": [
        "# apply one hot encoding\n",
        "encoding_cols = coupon_df.drop(columns=['Y']).columns\n",
        "coupon_data = np.concatenate([OneHotEncode(coupon_df, encoding_cols), coupon_df['Y'].to_numpy()[:, np.newaxis]], axis=1)\n",
        "coupon_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_evXd9ttETNF"
      },
      "source": [
        "## 6. Occupancy Detection Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0nr-tcBEW7J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d591bbcf-7c03-4b5b-f33e-e619ba3cb8c0"
      },
      "source": [
        "# read csv data from Github\n",
        "original_occupancy_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/occupancy.txt', sep=',', skipinitialspace=True)\n",
        "original_occupancy_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "CVhOjvo_VGXt",
        "outputId": "05e9c58c-5554-4a46-d69e-2a8959fb323d"
      },
      "source": [
        "# drop date column as it adds no value to the data\n",
        "occupancy_df = original_occupancy_df.drop(columns=['date'])\n",
        "occupancy_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2xwHpmsUFsl",
        "outputId": "9abcb35a-4f34-4031-fefd-69fe73caecca"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = occupancy_df.shape[0] - occupancy_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "TGDD49LqULE_",
        "outputId": "2b7c9a4a-e412-425f-ed71-f7b7c5f7541e"
      },
      "source": [
        "# plot coupon class balance\n",
        "plotBalance(occupancy_df, 'Occupancy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YFLXtT1VDaw",
        "outputId": "254de4ed-9684-4624-e907-ef6d86183b7c"
      },
      "source": [
        "# apply standard scaling\n",
        "scale_cols = occupancy_df.drop(columns=['Occupancy']).columns\n",
        "occupancy_data = np.concatenate([StandardScale(occupancy_df, scale_cols), occupancy_df['Occupancy'].to_numpy()[:, np.newaxis]], axis=1)\n",
        "occupancy_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzVC7viVEXmn"
      },
      "source": [
        "## 7. Default of credit card clients Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4eV8544EcEZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "bce0cf68-0911-4eb3-ff07-62885622fd51"
      },
      "source": [
        "# read csv data from Github\n",
        "original_credit_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/default%20of%20credit%20card%20clients.csv', sep=',', skipinitialspace=True)\n",
        "original_credit_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "pwU57yTuYYrA",
        "outputId": "3cc6b726-7025-49a7-bdd8-02313be83b24"
      },
      "source": [
        "# drop ID column as it adds no value to the data\n",
        "credit_df = original_credit_df.drop(columns=['ID'])\n",
        "credit_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSMbmtwXYl68",
        "outputId": "46e57ae3-0f40-4d70-a1fa-0535b1c8de12"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = credit_df.shape[0] - credit_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Masw7AVbYqpy",
        "outputId": "12599998-d7c3-4d63-fd18-b76d11e9e08a"
      },
      "source": [
        "# plot default payment class balance\n",
        "plotBalance(credit_df, 'default payment next month')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ1xJFHZaPvS",
        "outputId": "ea5281a2-bf0c-4f5c-8652-1a948939c338"
      },
      "source": [
        "# apply one hot encoding and standard scaling\n",
        "encode_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "scale_cols = [x for x in credit_df.columns if x not in encode_cols]\n",
        "scale_cols.remove('default payment next month')\n",
        "credit_data = np.concatenate([StandardScale(credit_df, scale_cols), OneHotEncode(credit_df, encode_cols), credit_df['default payment next month'].to_numpy()[:, np.newaxis]], axis=1)\n",
        "credit_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS_j3gkXEcVU"
      },
      "source": [
        "## 8. MoCap Hand Postures Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxBRmAEjEgTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "5bc98033-3166-4433-bb1b-df20c2cd9f7b"
      },
      "source": [
        "# read csv data from Github\n",
        "original_mocap_df = pd.read_csv('https://raw.githubusercontent.com/Hellboy1008/COGS118A_Final_Project/master/data/mocap.csv', na_values='?', sep=',', skipinitialspace=True)\n",
        "original_mocap_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0kekZewlGwW"
      },
      "source": [
        "# drop user column as it provides no additional value to the data\n",
        "mocap_df = original_mocap_df.drop(columns=['User'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spgeTkFoh2zu",
        "outputId": "5362b626-2cc4-4515-b170-6d57ba558abf"
      },
      "source": [
        "# find the number of rows with missing values\n",
        "missing_rows = mocap_df.shape[0] - mocap_df.dropna().shape[0]\n",
        "missing_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haLXEP8WiXGJ",
        "outputId": "aeb8a463-d96c-49be-f7bb-c343dd00f274"
      },
      "source": [
        "# find out which columns have missing values\n",
        "mocap_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "5GKYoeBijhan",
        "outputId": "c9776441-a615-4b9c-ae50-df7e6250cb1a"
      },
      "source": [
        "# drop the data from the columns with over 10000 instances of missing values\n",
        "mocap_df = mocap_df.drop(columns=['X5', 'Y5', 'Z5', 'X6', 'Y6', 'Z6', 'X7', 'Y7', 'Z7', 'X8', 'Y8', 'Z8', 'X9', 'Y9', 'Z9', 'X10', 'Y10', 'Z10', 'X11', 'Y11', 'Z11'])\n",
        "# drop the rest of the rows with missing values as it is only 3% of the dataset\n",
        "mocap_df = mocap_df.dropna()\n",
        "mocap_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "hWXs0H5elr-C",
        "outputId": "ac81b1a1-2013-42aa-a02b-47e08d0e8fbe"
      },
      "source": [
        "# combine class 3 and 4 for pointing (1) and combine class 1, 2, and 5 for not pointing (0)\n",
        "mocap_df['Class'] = mocap_df['Class'].replace([1,2,3,4,5], [0,0,1,1,0])\n",
        "mocap_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "HCAFO1dokswq",
        "outputId": "5aeb4f37-ee17-424e-ae18-95c3ce351a9d"
      },
      "source": [
        "# plot class balance\n",
        "plotBalance(mocap_df, 'Class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7WkRrsk88j",
        "outputId": "1ffc9a31-2d0f-4809-84e7-c5a65dea2dd8"
      },
      "source": [
        "# apply standard scaling\n",
        "scale_cols = mocap_df.drop(columns=['Class']).columns\n",
        "mocap_data = np.concatenate([StandardScale(mocap_df, scale_cols), mocap_df['Class'].to_numpy()[:, np.newaxis]], axis=1)\n",
        "mocap_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYggEHuGShPF"
      },
      "source": [
        "# Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f_tuSQ3Z25L"
      },
      "source": [
        "# function that writes data into a text file given the dataset and algorithm as the filename used to store all the raw data the 240 total trials\n",
        "def write(data, dataset, algorithm, description):\n",
        "    original_stdout = sys.stdout\n",
        "    with open('./results/' + dataset + '-' + algorithm + '-' +  description + '.result', 'a') as f:\n",
        "        sys.stdout = f\n",
        "        print(data)\n",
        "        sys.stdout = original_stdout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# runs each metric for the test set and returns a list containing the value for all the metrics\n",
        "def getMetrics(y_pred, y_true):\n",
        "    metrics_lst = []\n",
        "    metrics_lst.append(('accuracy', accuracy_score(y_true, y_pred)))\n",
        "    metrics_lst.append(('average-precision-score', average_precision_score(y_true, y_pred)))\n",
        "    metrics_lst.append(('f1', f1_score(y_true, y_pred)))\n",
        "    metrics_lst.append(('jaccard', jaccard_score(y_true, y_pred)))\n",
        "    metrics_lst.append(('recall', recall_score(y_true, y_pred)))\n",
        "    metrics_lst.append(('roc_auc_score', roc_auc_score(y_true, y_pred)))\n",
        "    return metrics_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# useful lists that are used later on for calculations\n",
        "datasets = [adult_data, mushroom_data, qsar_data, eeg_data, coupon_data, occupancy_data, credit_data, mocap_data]\n",
        "datasets_str = ['ADULT', 'MUSHROOM', 'QSAR', 'EEG', 'COUPON', 'OCCUPANCY', 'CREDIT', 'MOCAP']\n",
        "tests = ['rank_test_accuracy', 'rank_test_average_precision', 'rank_test_f1', 'rank_test_jaccard', 'rank_test_recall', 'rank_test_roc_auc']\n",
        "metrics = ['accuracy', 'f1', 'roc_auc', 'average_precision', 'recall', 'jaccard']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support Vector Machines\n",
        "def SVM_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    svm_param_grid = {'kernel': ['linear', 'rbf'], 'C': np.logspace(-4,2,7)}\n",
        "    svm_clf = GridSearchCV(SVC(), svm_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=1)\n",
        "    svm_best_model = svm_clf.fit(X_train, y_train)\n",
        "    write(svm_best_model.cv_results_, datasets_str[data_index], 'svm', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = svm_best_model.cv_results_['params'][np.argmin(svm_best_model.cv_results_[test])]\n",
        "        best_model = SVC(**best_params)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'svm', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        X = data[:, 0:-1]\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        SVM_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "def LGR_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    lgr_pipe = Pipeline([('classifier', LogisticRegression())])\n",
        "    lgr_param_grid = [{'classifier': [LogisticRegression(max_iter=5000)], 'classifier__solver': ['saga'], 'classifier__penalty': ['l1', 'l2'], 'classifier__C': np.logspace(-4, 4, 5)}, {'classifier': [LogisticRegression(max_iter=5000)], 'classifier__solver': ['lbfgs'], 'classifier__penalty': ['l2'], 'classifier__C': np.logspace(-4, 4, 5)},]\n",
        "    lgr_clf = GridSearchCV(lgr_pipe, lgr_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=3)\n",
        "    lgr_best_model = lgr_clf.fit(X_train, y_train)\n",
        "    write(lgr_best_model.cv_results_, datasets_str[data_index], 'lgr', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = lgr_best_model.cv_results_['params'][np.argmin(lgr_best_model.cv_results_[test])]\n",
        "        best_params = {'C': best_params['classifier__C'], 'solver': best_params['classifier__solver'], 'penalty': best_params['classifier__penalty']}\n",
        "        best_model = LogisticRegression(**best_params)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'lgr', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        X = data[:, 0:-1]\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        LGR_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K Nearest Neighbors\n",
        "def KNN_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    neigbors = np.arange(1, X_train.shape[1]+1, 4)\n",
        "    if data_index == 2:\n",
        "        neigbors = np.arange(1, int(math.sqrt(X_train.shape[1])+1), 4)\n",
        "    knn_param_grid = {'n_neighbors': neigbors, 'algorithm': ['ball_tree', 'kd_tree']}\n",
        "    knn_clf = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=1)\n",
        "    knn_best_model = knn_clf.fit(X_train, y_train)\n",
        "    write(knn_best_model.cv_results_, datasets_str[data_index], 'knn', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = knn_best_model.cv_results_['params'][np.argmin(knn_best_model.cv_results_[test])]\n",
        "        best_model = KNeighborsClassifier(**best_params)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'knn', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        KNN_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees\n",
        "def DT_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    depth = np.arange(1, 41)\n",
        "    dt_param_grid = {'criterion': ['gini', 'entropy'], 'max_features': ['sqrt', 'log2'], 'max_depth': depth}\n",
        "    dt_clf = GridSearchCV(DecisionTreeClassifier(random_state=(data_index+1)*10+trial_num), dt_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=1)\n",
        "    dt_best_model = dt_clf.fit(X_train, y_train)\n",
        "    write(dt_best_model.cv_results_, datasets_str[data_index], 'dt', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = dt_best_model.cv_results_['params'][np.argmin(dt_best_model.cv_results_[test])]\n",
        "        best_model = DecisionTreeClassifier(**best_params, random_state=(data_index+1)*10+trial_num)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'dt', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        X = data[:, 0:-1]\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        DT_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gaussian Naive Bayes\n",
        "def NB_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    nb_param_grid = {'var_smoothing': np.logspace(-11, 0, 12)}\n",
        "    nb_clf = GridSearchCV(GaussianNB(), nb_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=1)\n",
        "    nb_best_model = nb_clf.fit(X_train, y_train)\n",
        "    write(nb_best_model.cv_results_, datasets_str[data_index], 'nb', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = nb_best_model.cv_results_['params'][np.argmin(nb_best_model.cv_results_[test])]\n",
        "        best_model = GaussianNB(**best_params)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'nb', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        X = data[:, 0:-1]\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        NB_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forests\n",
        "def RF_test(X_train, X_test, y_train, y_test, trial_num, data_index):\n",
        "    depth = np.arange(1, 22, 3)\n",
        "    rf_param_grid = {'criterion': ['gini', 'entropy'], 'max_features': ['sqrt', 'log2'], 'n_estimators': [100, 200, 300], 'max_depth': depth}\n",
        "    rf_clf = GridSearchCV(RandomForestClassifier(random_state=(data_index+1)*10+trial_num), rf_param_grid, cv=StratifiedKFold(n_splits=5), scoring=metrics, refit=False, verbose=1)\n",
        "    rf_best_model = rf_clf.fit(X_train, y_train)\n",
        "    write(rf_best_model.cv_results_, datasets_str[data_index], 'rf', 'trial-' + str(trial_num + 1))\n",
        "    # find the six best models per metric and record the metrics on the test set\n",
        "    for test in tests:\n",
        "        best_params = rf_best_model.cv_results_['params'][np.argmin(rf_best_model.cv_results_[test])]\n",
        "        best_model = RandomForestClassifier(**best_params, random_state=(data_index+1)*10+trial_num)\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        write(getMetrics(y_pred, y_test), datasets_str[data_index], 'rf', 'trial-' + str(trial_num + 1))\n",
        "\n",
        "for data_index, data in enumerate(datasets):\n",
        "    for trial_num in range(5):\n",
        "        X = data[:, 0:-1]\n",
        "        y = data[:, -1:].flatten()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=((X.shape[0]-5000.0)/X.shape[0]), stratify=y)\n",
        "        RF_test(X_train, X_test, y_train, y_test, trial_num, data_index)"
      ]
    }
  ]
}